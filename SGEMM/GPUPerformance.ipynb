{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGEMM GPU kernel performance\n",
    "\n",
    "\n",
    "This data set measures the running time of a matrix-matrix product A*B = C, where all matrices have size 2048 x 2048, using a parameterizable SGEMM GPU kernel with 261400 possible parameter combinations. For each tested combination, 4 runs were performed and their results are reported as the 4 last columns. All times are measured in milliseconds\n",
    "      \n",
    "Attribute Information:<br>\n",
    "Independent variables:\n",
    "**MWG, NWG:** per-matrix 2D tiling at workgroup level: {16, 32, 64, 128} (integer)<br>\n",
    "3 **KWG:** inner dimension of 2D tiling at workgroup level: {16, 32} (integer)<br>\n",
    "4-5 **MDIMC, NDIMC:** local workgroup size: {8, 16, 32} (integer)<br>\n",
    "6-7 **MDIMA, NDIMB:** local memory shape: {8, 16, 32} (integer)<br>\n",
    "8 **KWI:** kernel loop unrolling factor: {2, 8} (integer)<br>\n",
    "9-10 **VWM, VWN:** per-matrix vector widths for loading and storing: {1, 2, 4, 8} (integer)<br>\n",
    "11-12 **STRM, STRN**: enable stride for accessing off-chip memory within a single thread: {0, 1} (categorical)<br>\n",
    "13-14 **SA, SB**: per-matrix manual caching of the 2D workgroup tile: {0, 1} (categorical)<br>\n",
    "\n",
    "Output:<br>\n",
    "15-18 **Run1, Run2, Run3, Run4:** performance times in milliseconds for 4 independent runs using the same parameters. They range between 13.25 and 3397.08.   \n",
    "\n",
    "Preprocessing:<br>\n",
    "Compute the mean of the four Output columns in one column \"Run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../RegressionAlgorithms/')\n",
    "import linearRegression\n",
    "from knn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sgemm_product.csv', delimiter = ',', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check for missing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Merge the Runs Columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data,columns=['Run1 (ms)','Run2 (ms)','Run3 (ms)', 'Run4 (ms)'])\n",
    "av_row = df.mean(axis=1)\n",
    "\n",
    "data.drop(columns=['Run1 (ms)','Run2 (ms)','Run3 (ms)', 'Run4 (ms)'])\n",
    "data['Run'] = av_row\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Count**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Histogram of Run Count Distribution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,5))\n",
    "sns.set_style('darkgrid')\n",
    "bins = np.arange(0, 3400, 100).tolist()\n",
    "data['Run'].hist(bins=bins)\n",
    "plt.xticks(bins)\n",
    "plt.xlabel('Run Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs MWG & NWG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot Run Time vs MWG & NWG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['MWG'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs KWG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs KWG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "sns.boxplot(x=data['KWG'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs MDIMC & NDIMC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs MDIMC & NDIMC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(x=data['NDIMC'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs MDIMA & NDIMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs MDIMA & NDIMA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['VWM'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs KWI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs KWI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['KWI'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs VWM & VWNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs VWM & VWN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['VWM'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs STRM & STRN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs STRM & STRN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['STRM'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs SA & SB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs SA & SB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['SA'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We don't have nominal value. No Encoding needed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Run', axis=1)\n",
    "y = data['Run']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training and Test Split*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data in attributes and class as well as training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Regression Algorithms from Sklearn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred1 = model.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', model.coef_)\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred1))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred1))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred1))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=3).fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Our Regression Algorithms*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "try:\n",
    "    del X_train['bias']\n",
    "except:\n",
    "    print('no bias to remove X_train')    \n",
    "try:\n",
    "    del X_test['bias']\n",
    "except:\n",
    "    print('no bias to remove X_test')\n",
    "try:\n",
    "    del X['bias']\n",
    "except:\n",
    "    print('no bias to remove X')\n",
    "\n",
    "\n",
    "import linearRegressionNumpy\n",
    "\n",
    "    \n",
    "#print('\\nMatrix Solution for Comparison:')\n",
    "#weightsMatrix = linearRegressionNumpy.matrixSolution(X_train, y_train)\n",
    "#yPredMatrix = linearRegressionNumpy.predictLinearRegression(X_test, weightsMatrix)\n",
    "#print('weights = ', weightsMatrix)\n",
    "\n",
    "\n",
    "print('\\n Now our own iterative regression algorithm:')    \n",
    "alpha = [] #[500, 500, 500, 500]\n",
    "alphaMethod = 'const'\n",
    "mu = 1\n",
    "convCritList = [1e5, 1e4, 1e3, 1e2, 1e1, 1e0, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "print('epsilon       | sum total error:   | sum relative error:  | iterations | Rsquare   ')\n",
    "for convergenceCriterion in convCritList:\n",
    "\n",
    "    weights, score, iterations = linearRegressionNumpy.linearRegression(X_train, y_train, alpha = alpha, mu = mu, \n",
    "                                                        convergenceCriterion = convergenceCriterion, lossFunction = 'MSE', \n",
    "                                                        alphaMethod = alphaMethod, printOutput = False)\n",
    "    yPred2 = linearRegressionNumpy.predictLinearRegression(X_test, weights)\n",
    "\n",
    "\n",
    "\n",
    "    print('{:13E} | {:19}| {:21}| {:11}| {:10}'.format(convergenceCriterion, \n",
    "                                        str(np.sum(yPred2-y_pred1)), \n",
    "                                        str(np.sum((yPred2-y_pred1)/y_pred1)),\n",
    "                                        str(iterations),\n",
    "                                        str(r2_score(y_test, yPred2))))\n",
    "    #print('sum total error: us iterative vs. scikit = ', np.sum(yPred2-y_pred1))\n",
    "    #print('sum relative error: us iterative vs. scikit = ', np.sum((yPred2-y_pred1)/y_pred1))\n",
    "\n",
    "print('yPred = ', yPred2)\n",
    "print('weights = ', weights)\n",
    "print('score = ', score)   \n",
    "    \n",
    "#print('\\n\\n\\n\\n\\n')\n",
    "\n",
    "plt.plot(y_pred1, label = 'scikit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(yPred2, label = 'us iterative solution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(yPredMatrix, label = 'us matrix solution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(yPred2-y_pred1, label = 'total error, us iterative vs. scikit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(yPredMatrix-y_pred1, label = 'total error, us matrix vs. scikit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(yPred2-yPredMatrix, label = 'total error, us iterative vs. us matrix')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot((yPred2-y_pred1)/y_pred1, label = 'relative error, us iterative vs. scikit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot((yPredMatrix-y_pred1)/y_pred1, label = 'relative error, us matrix vs. scikit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot((yPred2-yPredMatrix)/yPredMatrix, label = 'relative error, us vs. us matrix')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary creation to apply the mathematical functions of the algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = data.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forecasting instances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 1 # 1 = KNeighbors; 2 = RadiusNeighbors\n",
    "n_neighbours = 5\n",
    "distance_function = 1 # 1 = Euclidean Distance; 2 = Manhattan Distance\n",
    "radius = 0 # 0 indicates no radius\n",
    "label = 'Run'\n",
    "features = ['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(dictionary, label, features, mode, n_neighbours, distance_function, radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution of the algorithm (forecasting)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for x in y_test.index:\n",
    "    #print(x)\n",
    "    target = dictionary[x-1]\n",
    "    #print(target)\n",
    "    result = knn.run(target)\n",
    "    #print(result)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series(results,index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, predictions))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

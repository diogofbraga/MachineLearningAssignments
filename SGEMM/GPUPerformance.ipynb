{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGEMM GPU kernel performance\n",
    "\n",
    "\n",
    "This data set measures the running time of a matrix-matrix product A*B = C, where all matrices have size 2048 x 2048, using a parameterizable SGEMM GPU kernel with 261400 possible parameter combinations. For each tested combination, 4 runs were performed and their results are reported as the 4 last columns. All times are measured in milliseconds\n",
    "      \n",
    "Attribute Information:<br>\n",
    "Independent variables:\n",
    "**MWG, NWG:** per-matrix 2D tiling at workgroup level: {16, 32, 64, 128} (integer)<br>\n",
    "3 **KWG:** inner dimension of 2D tiling at workgroup level: {16, 32} (integer)<br>\n",
    "4-5 **MDIMC, NDIMC:** local workgroup size: {8, 16, 32} (integer)<br>\n",
    "6-7 **MDIMA, NDIMB:** local memory shape: {8, 16, 32} (integer)<br>\n",
    "8 **KWI:** kernel loop unrolling factor: {2, 8} (integer)<br>\n",
    "9-10 **VWM, VWN:** per-matrix vector widths for loading and storing: {1, 2, 4, 8} (integer)<br>\n",
    "11-12 **STRM, STRN**: enable stride for accessing off-chip memory within a single thread: {0, 1} (categorical)<br>\n",
    "13-14 **SA, SB**: per-matrix manual caching of the 2D workgroup tile: {0, 1} (categorical)<br>\n",
    "\n",
    "Output:<br>\n",
    "15-18 **Run1, Run2, Run3, Run4:** performance times in milliseconds for 4 independent runs using the same parameters. They range between 13.25 and 3397.08.   \n",
    "\n",
    "Preprocessing:<br>\n",
    "Compute the mean of the four Output columns in one column \"Run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing, datasets, linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../RegressionAlgorithms/')\n",
    "from knn import *\n",
    "import linearRegressionNumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sgemm_product.csv', delimiter = ',', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check for missing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Merge the Runs Columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data,columns=['Run1 (ms)','Run2 (ms)','Run3 (ms)', 'Run4 (ms)'])\n",
    "av_row = df.mean(axis=1)\n",
    "\n",
    "data.drop(columns=['Run1 (ms)','Run2 (ms)','Run3 (ms)', 'Run4 (ms)'])\n",
    "data['Run'] = av_row\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Count**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Histogram of Run Count Distribution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,5))\n",
    "sns.set_style('darkgrid')\n",
    "bins = np.arange(0, 3400, 100).tolist()\n",
    "data['Run'].hist(bins=bins)\n",
    "plt.xticks(bins)\n",
    "plt.xlabel('Run Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs MWG & NWG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot Run Time vs MWG & NWG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['MWG'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs KWG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs KWG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "sns.boxplot(x=data['KWG'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs MDIMC & NDIMC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs MDIMC & NDIMC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.boxplot(x=data['NDIMC'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs MDIMA & NDIMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs MDIMA & NDIMA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['VWM'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs KWI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs KWI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['KWI'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs VWM & VWNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs VWM & VWN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['VWM'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs STRM & STRN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs STRM & STRN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['STRM'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Time vs SA & SB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Box Plot on Run Time vs SA & SB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=data['SA'], y=data['Run'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We don't have nominal value, so no Encoding needed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Run', axis=1)\n",
    "y = data['Run']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training and Test Split*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in attributes and class as well as training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Regression Algorithms from Sklearn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = linear_model.LinearRegression().fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred1 = model.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cross validation score: ', cross_val_score(model, X_test, y_pred1, cv=10))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred1))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred1))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred1))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))\n",
    "print(\"Time: %0.2f\" % (end - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = KNeighborsRegressor(n_neighbors=3).fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cross validation score: ', cross_val_score(model, X_test, y_pred, cv=10))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Time: %0.2f\" % (end - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = DecisionTreeRegressor(random_state = 0).fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cross validation score: ', cross_val_score(model, X_test, y_pred, cv=10))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Time: %0.2f\" % (end - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = RandomForestRegressor().fit(X_train, y_train)\n",
    "end = time.time()\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cross validation score: ', cross_val_score(model, X_test, y_pred, cv=10))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Time: %0.2f\" % (end - start), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Regression Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Function (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del X_train['bias']\n",
    "except:\n",
    "    print('no bias to remove X_train')    \n",
    "try:\n",
    "    del X_test['bias']\n",
    "except:\n",
    "    print('no bias to remove X_test')\n",
    "try:\n",
    "    del X['bias']\n",
    "except:\n",
    "    print('no bias to remove X')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n GPU: Linear Regression Function (MSE):')    \n",
    "alphaMethod = 'const'\n",
    "mu = 1\n",
    "convCritList = [1e5, 1e4, 1e3, 1e2, 1e1, 1e0, 1e-1, 1e-2]\n",
    "print('epsilon       | sum total error:   | sum relative error:  | iterations | Rsquare |    time/s')\n",
    "for convergenceCriterion in convCritList:\n",
    "    start = time.time()\n",
    "    weights, score, iterations = linearRegressionNumpy.linearRegression(X_train, y_train, mu = mu, \n",
    "                                                        convergenceCriterion = convergenceCriterion, lossFunction = 'MSE', \n",
    "                                                        alphaMethod = alphaMethod, printOutput = False)\n",
    "    end = time.time()\n",
    "    yPred2 = linearRegressionNumpy.predictLinearRegression(X_test, weights)\n",
    "\n",
    "\n",
    "\n",
    "    print('{:13.0E} | {:19}| {:21}| {:11}| {:8.4f}| {:10.5f}'.format(convergenceCriterion, \n",
    "                                        str(np.sum(yPred2-y_pred1)), \n",
    "                                        str(np.sum((yPred2-y_pred1)/y_pred1)),\n",
    "                                        str(iterations),\n",
    "                                        r2_score(y_test, yPred2),\n",
    "                                        end-start))\n",
    "    \n",
    "print('\\nFinal weigths for smallest epsilon = {:2.0E}:'.format(convCritList[-1]))\n",
    "print('weights = ', weights, '\\n')\n",
    "\n",
    "plt.title('GPUPerformance: scikit prediction')\n",
    "plt.plot(y_pred1)\n",
    "plt.ylabel('performance time (ms)')\n",
    "plt.savefig('GPUPerformance_scikit_prediction_MSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction (MSE)')\n",
    "plt.plot(yPred2)\n",
    "plt.ylabel('performance time (ms)')\n",
    "plt.savefig('GPUPerformance_our_prediction_MSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction (MSE) vs. scikit prediction')\n",
    "plt.plot(yPred2-y_pred1)\n",
    "plt.ylabel('total error')\n",
    "plt.savefig('GPUPerformance_total_error_MSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction (MSE) vs. scikit prediction')\n",
    "plt.plot((yPred2-y_pred1)/y_pred1)\n",
    "plt.ylabel('relative error')\n",
    "plt.savefig('GPUPerformance_relative_error_MSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n GPU: Linear Regression Function (MSE):')\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' %r2_score(y_test, yPred2))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, yPred2))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, yPred2))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, yPred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Function (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del X_train['bias']\n",
    "except:\n",
    "    print('no bias to remove X_train')    \n",
    "try:\n",
    "    del X_test['bias']\n",
    "except:\n",
    "    print('no bias to remove X_test')\n",
    "try:\n",
    "    del X['bias']\n",
    "except:\n",
    "    print('no bias to remove X')\n",
    "\n",
    "\n",
    "print('\\n GPU: Linear Regression Function (MAE):')   \n",
    "alphaMethod = 'const'\n",
    "mu = 1\n",
    "convCritList = [1e5, 1e4, 1e3, 1e2, 1e1, 1e0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
    "print('epsilon       | sum total error:   | sum relative error:  | iterations | Rsquare |    time/s')\n",
    "for convergenceCriterion in convCritList:\n",
    "    start = time.time()\n",
    "    weights, score, iterations = linearRegressionNumpy.linearRegression(X_train, y_train, mu = mu, \n",
    "                                                        convergenceCriterion = convergenceCriterion, lossFunction = 'MAE', \n",
    "                                                        alphaMethod = alphaMethod, printOutput = False)\n",
    "    end = time.time()\n",
    "    yPred2 = linearRegressionNumpy.predictLinearRegression(X_test, weights)\n",
    "\n",
    "\n",
    "\n",
    "    print('{:13.0E} | {:19}| {:21}| {:11}| {:8.4f}| {:10.5f}'.format(convergenceCriterion, \n",
    "                                        str(np.sum(yPred2-y_pred1)), \n",
    "                                        str(np.sum((yPred2-y_pred1)/y_pred1)),\n",
    "                                        str(iterations),\n",
    "                                        r2_score(y_test, yPred2),\n",
    "                                        end-start))\n",
    "\n",
    "print('\\nFinal weigths for smallest epsilon = {:2.0E}:'.format(convCritList[-1]))\n",
    "print('weights = ', weights, '\\n')\n",
    "\n",
    "plt.title('GPUPerformance: scikit prediction')\n",
    "plt.plot(y_pred1)\n",
    "plt.ylabel('performance time (ms)')\n",
    "plt.savefig('GPUPerformance_scikit_prediction_MAE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction (MAE)')\n",
    "plt.plot(yPred2)\n",
    "plt.ylabel('performance time (ms)')\n",
    "plt.savefig('GPUPerformance_our_prediction_(MAE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction ((MAE) vs. scikit prediction')\n",
    "plt.plot(yPred2-y_pred1)\n",
    "plt.ylabel('total error')\n",
    "plt.savefig('GPUPerformance_total_error_(MAE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction ((MAE) vs. scikit prediction')\n",
    "plt.plot((yPred2-y_pred1)/y_pred1)\n",
    "plt.ylabel('relative error')\n",
    "plt.savefig('GPUPerformance_relative_error_(MAE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n GPU: Linear Regression Function (MAE):')\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, yPred2))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, yPred2))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, yPred2))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, yPred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Function (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del X_train['bias']\n",
    "except:\n",
    "    print('no bias to remove X_train')    \n",
    "try:\n",
    "    del X_test['bias']\n",
    "except:\n",
    "    print('no bias to remove X_test')\n",
    "try:\n",
    "    del X['bias']\n",
    "except:\n",
    "    print('no bias to remove X')\n",
    "\n",
    "\n",
    "print('\\n GPU: Linear Regression Function (RMSE):')\n",
    "alphaMethod = 'const'\n",
    "mu = 1\n",
    "convCritList = [1e5]#, 1e4, 1e3, 1e2, 1e1, 1e0, 1e-1, 1e-2, 1e-3, 1e-4]#, 1e-5]\n",
    "print('epsilon       | sum total error:   | sum relative error:  | iterations | Rsquare |    time/s')\n",
    "for convergenceCriterion in convCritList:\n",
    "    start = time.time()\n",
    "    weights, score, iterations = linearRegressionNumpy.linearRegression(X_train, y_train, mu = mu, \n",
    "                                                        convergenceCriterion = convergenceCriterion, lossFunction = 'RMSE', \n",
    "                                                        alphaMethod = alphaMethod, printOutput = False)\n",
    "    end = time.time()\n",
    "    yPred2 = linearRegressionNumpy.predictLinearRegression(X_test, weights)\n",
    "\n",
    "\n",
    "\n",
    "    print('{:13.0E} | {:19}| {:21}| {:11}| {:8.4f}| {:10.5f}'.format(convergenceCriterion, \n",
    "                                        str(np.sum(yPred2-y_pred1)), \n",
    "                                        str(np.sum((yPred2-y_pred1)/y_pred1)),\n",
    "                                        str(iterations),\n",
    "                                        r2_score(y_test, yPred2),\n",
    "                                        end-start))\n",
    "\n",
    "print('\\nFinal weigths for smallest epsilon = {:2.0E}:'.format(convCritList[-1]))\n",
    "print('weights = ', weights, '\\n')\n",
    "\n",
    "plt.title('GPUPerformance: scikit prediction')\n",
    "plt.plot(y_pred1)\n",
    "plt.ylabel('performance time(ms)')\n",
    "plt.savefig('GPUPerformance_scikit_prediction_RMSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('SeoulBikeSharing: our prediction (RMSE)')\n",
    "plt.plot(yPred2)\n",
    "plt.ylabel('performance time (ms)')\n",
    "plt.savefig('GPUPerformance_our_prediction_RMSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction (RMSE) vs. scikit prediction')\n",
    "plt.plot(yPred2-y_pred1)\n",
    "plt.ylabel('total error')\n",
    "plt.savefig('GPUPerformance_total_error_RMSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.title('GPUPerformance: our prediction (RMSE) vs. scikit prediction')\n",
    "plt.plot((yPred2-y_pred1)/y_pred1)\n",
    "plt.ylabel('relative error')\n",
    "plt.savefig('GPUPerformance_relative_error_RMSE.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n GPU: Linear Regression Function (RMSE):')\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, yPred2))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, yPred2))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, yPred2))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, yPred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary creation to apply the mathematical functions of the algorithm**\n",
    "\n",
    "Training Data Option:\n",
    "- 0: All Data (except the target)\n",
    "- 1: X_train/y_train (train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_option = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_data_option == 0:\n",
    "    training_data = data\n",
    "elif training_data_option == 1:\n",
    "    training_data = data[data.index.isin(X_train.index)]\n",
    "    test_data = data[data.index.isin(X_test.index)]\n",
    "    \n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_data_option == 0:\n",
    "    training_dictionary = training_data.to_dict('records')\n",
    "elif training_data_option == 1:\n",
    "    training_dictionary = training_data.to_dict('records')\n",
    "    test_dictionary = test_data.to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forecasting instances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 1 # 1 = KNeighbors; 2 = RadiusNeighbors\n",
    "n_neighbours = 5\n",
    "distance_function = 1 # 1 = Euclidean Distance; 2 = Manhattan Distance\n",
    "radius = 0 # 0 indicates no radius\n",
    "label = 'Run'\n",
    "features = ['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(training_dictionary, label, features, mode, n_neighbours, distance_function, radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution of the algorithm (forecasting)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if training_data_option == 0:\n",
    "    for x in y_test.index:\n",
    "        #print(x)\n",
    "        target = training_dictionary[x-1]\n",
    "        #print(target)\n",
    "        result = knn.run(target)\n",
    "        #print(result)\n",
    "        results.append(result)\n",
    "elif training_data_option == 1:\n",
    "    for x in y_test.index:\n",
    "        #print(x)\n",
    "        target = test_dictionary[x]\n",
    "        #print(target)\n",
    "        result = knn.run(target)\n",
    "        #print(result)\n",
    "        results.append(result)\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.Series(results,index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, predictions))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "print(\"Time: %0.2f\" % (end - start), \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
